{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBbuYyA_xlAr",
        "outputId": "1276d2b1-1953-4e8f-af1d-2c1740031bb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 0.0, False, {'prob': 0.3333333333333333})\n",
            "Average reward with alpha=0.5, gamma=0.95, epsilon=1.0: 0.266\n",
            "Final Q-table with alpha=0.5, gamma=0.95, epsilon=1.0:\n",
            " [[0.22932384 0.10083721 0.10328055 0.13622133]\n",
            " [0.01269071 0.05294783 0.02202229 0.15615323]\n",
            " [0.03170875 0.12414031 0.06358228 0.03126253]\n",
            " [0.02458295 0.05133449 0.01601655 0.08360728]\n",
            " [0.34682062 0.10111006 0.04595797 0.05700287]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.00883567 0.02587999 0.23990195 0.00375247]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.150228   0.06167107 0.13258917 0.40187932]\n",
            " [0.24043033 0.49311575 0.07928502 0.04916282]\n",
            " [0.59003029 0.15711064 0.04796942 0.01864821]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.10690629 0.14501292 0.49079914 0.04887385]\n",
            " [0.35435195 0.80533881 0.3441339  0.41291857]\n",
            " [0.         0.         0.         0.        ]]\n",
            "\n",
            "Average reward with alpha=0.5, gamma=0.95, epsilon=0.5: 0.3288\n",
            "Final Q-table with alpha=0.5, gamma=0.95, epsilon=0.5:\n",
            " [[0.09737586 0.19111372 0.03281169 0.03764453]\n",
            " [0.02320771 0.02258901 0.01262605 0.21050747]\n",
            " [0.15680442 0.17656795 0.01815251 0.02299691]\n",
            " [0.00911689 0.01968868 0.01334991 0.08128975]\n",
            " [0.18942628 0.02089805 0.03888095 0.02015517]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.00697425 0.0047362  0.06382683 0.00806297]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.16238031 0.06073118 0.06132139 0.36907431]\n",
            " [0.05016704 0.61539828 0.078169   0.07931919]\n",
            " [0.75477644 0.02244719 0.11584241 0.01751103]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.1383316  0.12114422 0.52667021 0.08414404]\n",
            " [0.22371376 0.89866715 0.25885948 0.28524757]\n",
            " [0.         0.         0.         0.        ]]\n",
            "\n",
            "Average reward with alpha=0.5, gamma=0.95, epsilon=0.1: 0.0\n",
            "Final Q-table with alpha=0.5, gamma=0.95, epsilon=0.1:\n",
            " [[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "\n",
            "Average reward with alpha=0.5, gamma=0.8, epsilon=1.0: 0.161\n",
            "Final Q-table with alpha=0.5, gamma=0.8, epsilon=1.0:\n",
            " [[1.43459783e-03 1.40765629e-03 1.04838769e-02 1.38652146e-03]\n",
            " [4.01906733e-04 1.03480669e-03 5.49578567e-03 9.21835942e-03]\n",
            " [2.99522273e-02 1.14666324e-03 1.48019746e-03 1.39508722e-03]\n",
            " [3.15287087e-04 2.42926587e-04 3.02079649e-04 1.30604497e-03]\n",
            " [2.69928396e-02 8.90610948e-04 1.73584817e-03 1.84959977e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [2.11122717e-01 8.74261375e-05 3.39082551e-04 5.52516030e-04]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.36474577e-03 3.05754504e-03 2.78238468e-04 3.16167451e-02]\n",
            " [9.92755945e-03 1.04877421e-01 4.11853734e-03 1.35217343e-02]\n",
            " [2.80626874e-01 9.40340077e-03 8.17236642e-03 2.28183754e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [2.07932565e-02 1.45797642e-02 5.76847442e-01 1.60440219e-01]\n",
            " [8.20355542e-02 9.30725819e-02 8.06966485e-01 1.46082107e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "\n",
            "Average reward with alpha=0.5, gamma=0.8, epsilon=0.5: 0.1852\n",
            "Final Q-table with alpha=0.5, gamma=0.8, epsilon=0.5:\n",
            " [[6.30166740e-04 2.60744885e-02 7.09782538e-03 7.05653313e-03]\n",
            " [2.39119571e-04 1.91105194e-03 2.41686199e-04 1.75903493e-02]\n",
            " [2.33557074e-04 2.22582024e-04 1.01704832e-02 2.65292138e-04]\n",
            " [1.21240166e-04 9.59803709e-05 1.71482321e-04 6.60395524e-03]\n",
            " [1.11813027e-03 8.93587737e-04 1.69085130e-02 1.23076820e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [8.46670725e-05 3.39539996e-05 7.38850859e-03 6.84329335e-05]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [9.94961952e-03 2.38726047e-03 9.98075522e-03 1.45430489e-01]\n",
            " [2.04181697e-02 2.03106645e-01 2.47355946e-02 2.67832005e-02]\n",
            " [1.95299221e-01 1.70433839e-02 4.03452456e-02 1.37535197e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [3.12619688e-02 1.38895787e-02 4.81303057e-01 4.41848833e-03]\n",
            " [1.37265541e-01 1.74029442e-01 1.35446428e-01 9.50124269e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "\n",
            "Average reward with alpha=0.5, gamma=0.8, epsilon=0.1: 0.0\n",
            "Final Q-table with alpha=0.5, gamma=0.8, epsilon=0.1:\n",
            " [[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "\n",
            "Average reward with alpha=0.5, gamma=0.6, epsilon=1.0: 0.0942\n",
            "Final Q-table with alpha=0.5, gamma=0.6, epsilon=1.0:\n",
            " [[2.39774659e-05 3.75338450e-05 1.66721362e-04 2.14911997e-05]\n",
            " [1.10421581e-05 2.48312862e-05 1.06076881e-05 3.95341515e-04]\n",
            " [8.91088978e-04 1.27731812e-05 1.10324659e-05 1.17523068e-05]\n",
            " [1.31331110e-05 8.45709269e-06 8.50682858e-06 1.08516026e-05]\n",
            " [9.11520097e-03 1.52862940e-05 3.93024702e-05 7.12270781e-06]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [9.69134516e-06 1.96065869e-06 1.43862922e-02 7.41425360e-06]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [9.85065975e-05 1.49524391e-02 5.44056556e-04 4.65508228e-04]\n",
            " [2.33534096e-03 3.39945400e-02 1.61897829e-03 1.74912241e-03]\n",
            " [4.81325703e-02 6.00318549e-03 8.66662923e-04 1.15335363e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [6.83615170e-03 2.89374427e-03 6.94534202e-02 5.48584942e-03]\n",
            " [4.62056873e-02 5.32370519e-02 3.40740638e-01 2.74313471e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "\n",
            "Average reward with alpha=0.5, gamma=0.6, epsilon=0.5: 0.0\n",
            "Final Q-table with alpha=0.5, gamma=0.6, epsilon=0.5:\n",
            " [[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "\n",
            "Average reward with alpha=0.5, gamma=0.6, epsilon=0.1: 0.0\n",
            "Final Q-table with alpha=0.5, gamma=0.6, epsilon=0.1:\n",
            " [[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "\n",
            "Average reward with alpha=0.8, gamma=0.95, epsilon=1.0: 0.2178\n",
            "Final Q-table with alpha=0.8, gamma=0.95, epsilon=1.0:\n",
            " [[3.57537557e-01 4.99863588e-02 5.19177256e-02 1.16782307e-01]\n",
            " [4.41194839e-03 1.32511715e-03 3.13692544e-04 2.96261672e-01]\n",
            " [2.63118945e-03 3.26092917e-03 6.49952916e-03 3.96452755e-02]\n",
            " [2.24863419e-03 1.00319870e-03 4.59231025e-03 1.41272838e-02]\n",
            " [5.00545386e-01 6.19185068e-02 5.50008833e-02 5.72878911e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [4.43106671e-05 9.19885625e-09 6.18032455e-03 2.82973362e-05]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [2.91322134e-03 8.18145383e-03 1.16990244e-01 6.54534279e-01]\n",
            " [3.72115419e-03 6.78174656e-01 5.51015669e-03 1.02652862e-02]\n",
            " [1.63978566e-01 2.03694146e-03 7.34086877e-03 6.13249199e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.04865514e-02 1.67222513e-02 9.08413280e-01 3.55343869e-02]\n",
            " [1.54957771e-01 9.96211773e-01 7.11111885e-02 2.01016206e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "\n",
            "Average reward with alpha=0.8, gamma=0.95, epsilon=0.5: 0.284\n",
            "Final Q-table with alpha=0.8, gamma=0.95, epsilon=0.5:\n",
            " [[6.02586409e-01 1.93021391e-02 1.84072754e-02 1.86102785e-02]\n",
            " [4.45504358e-03 2.36521179e-03 2.48766171e-03 9.79944764e-02]\n",
            " [8.52276711e-02 4.66730688e-03 6.62534607e-03 6.71653202e-03]\n",
            " [2.02491195e-03 1.31152090e-03 1.45433724e-03 6.50668667e-03]\n",
            " [3.84270972e-01 4.64125504e-03 1.47408781e-02 4.72763701e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [8.34329827e-03 3.43783922e-07 1.05682388e-04 1.82210069e-07]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [6.32469190e-03 2.57271784e-03 2.41605146e-02 1.22841655e-01]\n",
            " [1.05732745e-02 6.81299980e-01 5.50379928e-03 1.89700676e-03]\n",
            " [2.62478021e-03 8.00117761e-04 1.79753854e-01 1.48658641e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [6.90983627e-02 5.33512741e-02 5.75191823e-01 2.16142629e-02]\n",
            " [6.68133264e-02 9.84926081e-01 5.84311972e-02 1.39550148e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "\n",
            "Average reward with alpha=0.8, gamma=0.95, epsilon=0.1: 0.0\n",
            "Final Q-table with alpha=0.8, gamma=0.95, epsilon=0.1:\n",
            " [[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "\n",
            "Average reward with alpha=0.8, gamma=0.8, epsilon=1.0: 0.1532\n",
            "Final Q-table with alpha=0.8, gamma=0.8, epsilon=1.0:\n",
            " [[4.73679556e-04 1.84088385e-02 3.76320096e-04 9.90354715e-03]\n",
            " [6.38925233e-06 6.90664887e-05 5.21967082e-06 2.47257318e-03]\n",
            " [4.31583907e-05 7.06647920e-04 4.43846327e-05 3.98614586e-05]\n",
            " [1.90238249e-06 4.28987691e-05 1.04017272e-06 2.82944745e-06]\n",
            " [1.51113739e-02 1.61630905e-04 2.27151332e-04 2.35569262e-04]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.07260259e-05 5.09184420e-08 2.77517048e-02 6.61551387e-08]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [2.00915261e-03 1.43746545e-03 3.32137731e-03 7.41928218e-03]\n",
            " [7.58625082e-04 7.82994304e-02 5.97190588e-04 1.91802073e-05]\n",
            " [3.92312794e-04 3.30853482e-03 1.02654888e-03 8.96231446e-04]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [4.52535378e-02 5.66795522e-03 7.25262582e-01 2.91882338e-03]\n",
            " [2.48609924e-02 2.18403997e-02 1.95382745e-02 9.92824789e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "\n",
            "Average reward with alpha=0.8, gamma=0.8, epsilon=0.5: 0.196\n",
            "Final Q-table with alpha=0.8, gamma=0.8, epsilon=0.5:\n",
            " [[1.86829762e-02 4.47610800e-03 1.46682688e-03 9.86749741e-04]\n",
            " [6.76427354e-05 6.82426853e-04 4.25813830e-06 2.09500578e-03]\n",
            " [7.17030606e-05 1.29914591e-03 8.75614247e-05 8.74644015e-05]\n",
            " [1.96567475e-06 5.18028746e-06 4.15199768e-06 3.93776281e-03]\n",
            " [2.44081839e-02 1.19401460e-02 2.29898049e-04 5.40145340e-04]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.67077896e-02 9.70138652e-07 1.26127356e-06 5.89617832e-15]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.72230210e-06 4.81469073e-04 5.96699855e-04 2.44151498e-01]\n",
            " [8.40073116e-03 5.41897885e-01 6.87874848e-03 4.73492333e-03]\n",
            " [4.13498673e-01 7.42242523e-04 1.12244870e-03 1.41657330e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [2.87812420e-02 1.63867806e-04 5.60738874e-01 3.71015096e-03]\n",
            " [1.96294767e-02 9.40577143e-01 1.70251368e-02 9.43644624e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "\n",
            "Average reward with alpha=0.8, gamma=0.8, epsilon=0.1: 0.0\n",
            "Final Q-table with alpha=0.8, gamma=0.8, epsilon=0.1:\n",
            " [[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "\n",
            "Average reward with alpha=0.8, gamma=0.6, epsilon=1.0: 0.1008\n",
            "Final Q-table with alpha=0.8, gamma=0.6, epsilon=1.0:\n",
            " [[3.97646519e-06 9.27059773e-04 2.31852817e-06 2.47241906e-06]\n",
            " [1.66387185e-08 2.21995185e-07 2.62341233e-07 9.30496057e-05]\n",
            " [1.86872429e-04 1.33823278e-07 1.07049957e-07 6.94192182e-08]\n",
            " [7.94602161e-10 8.85320212e-09 4.85859027e-09 1.76409639e-08]\n",
            " [5.37688281e-04 5.73621000e-07 5.35376435e-07 2.66775139e-04]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [3.74833133e-07 1.37172573e-09 2.28201546e-04 6.14372859e-15]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [3.23965082e-05 1.75019073e-05 1.13996553e-05 1.05689361e-02]\n",
            " [4.01725964e-05 4.59115640e-02 1.82167538e-04 5.02234263e-04]\n",
            " [2.16897359e-03 3.29385217e-05 4.76507640e-05 2.84141729e-05]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.07431260e-04 1.90775018e-03 4.76591621e-01 3.07891704e-05]\n",
            " [5.65884636e-04 7.37423154e-03 9.32576452e-01 4.35780615e-04]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "\n",
            "Average reward with alpha=0.8, gamma=0.6, epsilon=0.5: 0.0\n",
            "Final Q-table with alpha=0.8, gamma=0.6, epsilon=0.5:\n",
            " [[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "\n",
            "Average reward with alpha=0.8, gamma=0.6, epsilon=0.1: 0.0\n",
            "Final Q-table with alpha=0.8, gamma=0.6, epsilon=0.1:\n",
            " [[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "\n",
            "Average reward with alpha=0.1, gamma=0.95, epsilon=1.0: 0.346\n",
            "Final Q-table with alpha=0.1, gamma=0.95, epsilon=1.0:\n",
            " [[0.19601838 0.14653163 0.14484994 0.14613418]\n",
            " [0.09248729 0.09438137 0.10614806 0.16178737]\n",
            " [0.14732493 0.09397987 0.09998624 0.09218873]\n",
            " [0.02626257 0.05986721 0.02211629 0.03975012]\n",
            " [0.23995202 0.13637187 0.13693243 0.14769084]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.08866557 0.06599207 0.20315316 0.03623394]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.21253254 0.2319696  0.20514848 0.32512421]\n",
            " [0.17119432 0.43250631 0.17136237 0.24969526]\n",
            " [0.56540184 0.2610211  0.19510405 0.16666119]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.34084022 0.40282162 0.55082256 0.2730485 ]\n",
            " [0.5325378  0.81960048 0.63959699 0.55216077]\n",
            " [0.         0.         0.         0.        ]]\n",
            "\n",
            "Average reward with alpha=0.1, gamma=0.95, epsilon=0.5: 0.422\n",
            "Final Q-table with alpha=0.1, gamma=0.95, epsilon=0.5:\n",
            " [[0.15037745 0.16937174 0.12479207 0.13509294]\n",
            " [0.0991648  0.08391048 0.09386418 0.14584876]\n",
            " [0.17486349 0.09447583 0.09208144 0.09356274]\n",
            " [0.00430283 0.04411179 0.01227845 0.00244373]\n",
            " [0.21931851 0.11994721 0.12262768 0.10061869]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.1240172  0.09641605 0.20348613 0.05257303]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.11407868 0.19295434 0.20177628 0.28189815]\n",
            " [0.15615711 0.40337005 0.26294278 0.27575255]\n",
            " [0.43805899 0.23984899 0.24046684 0.19161036]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.32921565 0.37963336 0.47308492 0.29186911]\n",
            " [0.50590208 0.57721311 0.47769541 0.43259051]\n",
            " [0.         0.         0.         0.        ]]\n",
            "\n",
            "Average reward with alpha=0.1, gamma=0.95, epsilon=0.1: 0.0\n",
            "Final Q-table with alpha=0.1, gamma=0.95, epsilon=0.1:\n",
            " [[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "\n",
            "Average reward with alpha=0.1, gamma=0.8, epsilon=1.0: 0.1906\n",
            "Final Q-table with alpha=0.1, gamma=0.8, epsilon=1.0:\n",
            " [[0.00881466 0.01668586 0.00849795 0.00900209]\n",
            " [0.00690535 0.00635417 0.00788248 0.01496043]\n",
            " [0.02571446 0.01089522 0.01475103 0.01249419]\n",
            " [0.00733122 0.00730641 0.00668104 0.0102135 ]\n",
            " [0.03144966 0.00834484 0.00724684 0.00944912]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.05674871 0.02039754 0.01819612 0.00583785]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.01949306 0.01955414 0.01690712 0.05820912]\n",
            " [0.07408763 0.1459389  0.06272404 0.06441431]\n",
            " [0.24270513 0.09192587 0.0846918  0.0623468 ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.09150042 0.15978041 0.29242911 0.10580487]\n",
            " [0.23515965 0.74116563 0.31767234 0.28099329]\n",
            " [0.         0.         0.         0.        ]]\n",
            "\n",
            "Average reward with alpha=0.1, gamma=0.8, epsilon=0.5: 0.2344\n",
            "Final Q-table with alpha=0.1, gamma=0.8, epsilon=0.5:\n",
            " [[0.00957337 0.00998715 0.01059573 0.00807748]\n",
            " [0.00719528 0.00967696 0.00907678 0.01026477]\n",
            " [0.01739979 0.01108273 0.01346327 0.01265137]\n",
            " [0.0082468  0.00621846 0.0060253  0.01165645]\n",
            " [0.01443667 0.00971517 0.01607118 0.00630308]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.02263795 0.01400064 0.04137395 0.01416757]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.02057382 0.02078762 0.02313821 0.04054862]\n",
            " [0.05205687 0.09144007 0.05214556 0.05132798]\n",
            " [0.12974082 0.06848392 0.08802611 0.05479994]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.11383119 0.11065959 0.1342102  0.12153004]\n",
            " [0.3062016  0.53539438 0.27808318 0.29925165]\n",
            " [0.         0.         0.         0.        ]]\n",
            "\n",
            "Average reward with alpha=0.1, gamma=0.8, epsilon=0.1: 0.0\n",
            "Final Q-table with alpha=0.1, gamma=0.8, epsilon=0.1:\n",
            " [[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "\n",
            "Average reward with alpha=0.1, gamma=0.6, epsilon=1.0: 0.093\n",
            "Final Q-table with alpha=0.1, gamma=0.6, epsilon=1.0:\n",
            " [[4.91714284e-04 6.59653663e-04 4.45605861e-04 4.64524180e-04]\n",
            " [4.19774420e-04 5.06994186e-04 3.81209034e-04 8.52013709e-04]\n",
            " [1.34067041e-03 1.27169826e-03 3.91172683e-03 1.29227377e-03]\n",
            " [5.20796415e-04 4.91565914e-04 5.08557715e-04 7.43374574e-04]\n",
            " [8.35682732e-04 6.73654395e-04 1.32929208e-03 8.17139008e-04]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.82024111e-02 4.81083528e-03 4.78383142e-03 1.76443638e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.73250104e-03 3.71625542e-03 8.57844674e-03 3.78021543e-03]\n",
            " [2.19209669e-02 3.71226108e-02 1.46242059e-02 1.78245799e-02]\n",
            " [8.31622386e-02 3.26429597e-02 3.17059623e-02 1.84847939e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [3.37644120e-02 4.22986904e-02 3.96630127e-02 9.57447863e-02]\n",
            " [1.41574858e-01 4.78241368e-01 2.10026413e-01 1.90672557e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "\n",
            "Average reward with alpha=0.1, gamma=0.6, epsilon=0.5: 0.0\n",
            "Final Q-table with alpha=0.1, gamma=0.6, epsilon=0.5:\n",
            " [[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "\n",
            "Average reward with alpha=0.1, gamma=0.6, epsilon=0.1: 0.0\n",
            "Final Q-table with alpha=0.1, gamma=0.6, epsilon=0.1:\n",
            " [[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "\n",
            "Policy Probability Distribution:\n",
            "[[1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\n",
            "[[0 3 3 3]\n",
            " [0 0 0 0]\n",
            " [3 1 0 0]\n",
            " [0 2 1 0]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average reward with Q-learning: 0.0\n",
            "Average reward with policy iteration: 0.0\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize the \"FrozenLake\" environment\n",
        "env = gym.make(\"FrozenLake-v1\")\n",
        "env.reset()\n",
        "print(env.step(env.action_space.sample()))\n",
        "\n",
        "# Set the number of actions and states\n",
        "num_actions = env.action_space.n\n",
        "num_states = env.observation_space.n\n",
        "\n",
        "# Initialize Q-table with zeros\n",
        "Q = np.zeros([num_states, num_actions])\n",
        "\n",
        "# Define hyperparameters\n",
        "alphas = [0.5, 0.8, 0.1]\n",
        "gammas = [0.95, 0.8, 0.6]\n",
        "epsilons = [1.0, 0.5, 0.1]\n",
        "# epsilon = 1.0\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.999\n",
        "total_episodes = 5000\n",
        "\n",
        "# Initialize lists to store results\n",
        "average_rewards = []\n",
        "final_Q_tables = []\n",
        "\n",
        "# Loop over all combinations of hyperparameters\n",
        "for alpha in alphas:\n",
        "    for gamma in gammas:\n",
        "        for epsilon in epsilons:\n",
        "            # Initialize Q-table with zeros\n",
        "            Q = np.zeros([num_states, num_actions])\n",
        "\n",
        "            # List to store rewards\n",
        "            rewards = []\n",
        "\n",
        "            # Start Q-learning process\n",
        "            for episode in range(total_episodes):\n",
        "                # Reset the environment\n",
        "                state = env.reset()\n",
        "                done = False\n",
        "                total_rewards = 0\n",
        "\n",
        "                for step in range(100):  # Limit each episode to a maximum of 100 steps\n",
        "                    # Choose action: random for exploration and max Q for exploitation\n",
        "                    if random.uniform(0, 1) < epsilon:\n",
        "                        action = env.action_space.sample()  # Explore action space\n",
        "                    else:\n",
        "                        action = np.argmax(Q[state, :])  # Exploit learned values\n",
        "\n",
        "                    # Perform the action and get the reward\n",
        "                    new_state, reward, done, info = env.step(action)\n",
        "\n",
        "                    # Update Q-table for Q(s,a)\n",
        "                    Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[new_state, :]) - Q[state, action])\n",
        "\n",
        "                    total_rewards += reward  # Add reward to total rewards\n",
        "\n",
        "                    state = new_state  # Update the state\n",
        "\n",
        "                    # If done, then finish the episode\n",
        "                    if done:\n",
        "                        break\n",
        "\n",
        "                # Decay epsilon to reduce exploration as time progresses\n",
        "                epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "                # Store total reward of each episode\n",
        "                rewards.append(total_rewards)\n",
        "\n",
        "            # Store average reward and final Q-table for each set of hyperparameters\n",
        "            average_rewards.append(sum(rewards) / total_episodes)\n",
        "            final_Q_tables.append(Q)\n",
        "\n",
        "# Display the results\n",
        "for i in range(len(average_rewards)):\n",
        "    print(f\"Average reward with alpha={alphas[i//9]}, gamma={gammas[(i//3)%3]}, epsilon={epsilons[i%3]}: {average_rewards[i]}\")\n",
        "    print(f\"Final Q-table with alpha={alphas[i//9]}, gamma={gammas[(i//3)%3]}, epsilon={epsilons[i%3]}:\\n {final_Q_tables[i]}\\n\")\n",
        "\n",
        "from gym.envs.toy_text.frozen_lake import FrozenLakeEnv\n",
        "\n",
        "def policy_evaluation(policy, env, discount_factor=1.0, theta=1e-9, max_iterations=1e9):\n",
        "    V = np.zeros(env.observation_space.n)\n",
        "    for i in range(int(max_iterations)):\n",
        "        delta = 0\n",
        "        for state in range(env.observation_space.n):\n",
        "            v = 0\n",
        "            for action, action_prob in enumerate(policy[state]):\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    v += action_prob * prob * (reward + discount_factor * V[next_state])\n",
        "            delta = max(delta, np.abs(V[state] - v))\n",
        "            V[state] = v\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return V\n",
        "\n",
        "def policy_improvement(env, policy_eval_fn=policy_evaluation, discount_factor=1.0):\n",
        "    def one_step_lookahead(state, V):\n",
        "        A = np.zeros(env.action_space.n)\n",
        "        for a in range(env.action_space.n):\n",
        "            for prob, next_state, reward, done in env.P[state][a]:\n",
        "                A[a] += prob * (reward + discount_factor * V[next_state])\n",
        "        return A\n",
        "\n",
        "    policy = np.ones([env.observation_space.n, env.action_space.n]) / env.action_space.n\n",
        "    while True:\n",
        "        V = policy_eval_fn(policy, env, discount_factor)\n",
        "        policy_stable = True\n",
        "        for state in range(env.observation_space.n):\n",
        "            chosen_a = np.argmax(policy[state])\n",
        "            action_values = one_step_lookahead(state, V)\n",
        "            best_a = np.argmax(action_values)\n",
        "            if chosen_a != best_a:\n",
        "                policy_stable = False\n",
        "            policy[state] = np.eye(env.action_space.n)[best_a]\n",
        "        if policy_stable:\n",
        "            return policy, V\n",
        "env = FrozenLakeEnv(map_name=\"4x4\", is_slippery=True)\n",
        "policy, v = policy_improvement(env)\n",
        "\n",
        "print(\"Policy Probability Distribution:\")\n",
        "print(policy)\n",
        "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
        "print(np.reshape(np.argmax(policy, axis=1), env.desc.shape))\n",
        "\n",
        "def run_episode(env, policy):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = np.argmax(policy[state])\n",
        "        # print(f\"Action: {action}\")\n",
        "        result = env.step(action)\n",
        "        # print(f\"Result: {result}\")\n",
        "        state, reward, done, _, _ = result  # Change this line\n",
        "        total_reward += reward\n",
        "    # print(total_reward)\n",
        "    return total_reward\n",
        "\n",
        "\n",
        "# Run the policy obtained from Q-learning\n",
        "q_learning_rewards = [run_episode(env, np.argmax(Q, axis=1)) for _ in range(1000)]\n",
        "average_q_learning_reward = np.mean(q_learning_rewards)\n",
        "\n",
        "# Run the policy obtained from policy iteration\n",
        "policy_iteration_rewards = [run_episode(env, np.argmax(policy, axis=1)) for _ in range(1000)]\n",
        "average_policy_iteration_reward = np.mean(policy_iteration_rewards)\n",
        "\n",
        "# Plot the rewards\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.hist(q_learning_rewards, alpha=0.5, label='Q-Learning', bins=20)\n",
        "plt.hist(policy_iteration_rewards, alpha=0.5, label='Policy Iteration', bins=20)\n",
        "plt.xlabel('Rewards')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Comparison of Rewards between Q-Learning and Policy Iteration')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Average reward with Q-learning: {average_q_learning_reward}\")\n",
        "print(f\"Average reward with policy iteration: {average_policy_iteration_reward}\")# Plot the rewards\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym[atari]\n",
        "!pip install gym --upgrade\n",
        "\n",
        "\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from gym.wrappers import AtariPreprocessing\n",
        "\n",
        "# env_names = gym.envs.registry.env_specs.keys()\n",
        "# print(env_names)\n",
        "# print(gym.envs.registry.all())\n",
        "\n",
        "\n",
        "class DQN:\n",
        "    def __init__(self, state_shape, n_actions, lr=0.00025, gamma=0.95, epsilon_start=1.0, epsilon_min=0.01,\n",
        "                 epsilon_decay=0.995, memory_size=10000, batch_size=32):\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.memory = []\n",
        "        self.memory_size = memory_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.model = self._build_network(state_shape, lr)\n",
        "        self.target_model = self._build_network(state_shape, lr)\n",
        "        self.sync_target()\n",
        "\n",
        "    def _build_network(self, state_shape, lr):\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=state_shape),\n",
        "            tf.keras.layers.Conv2D(64, (4, 4), strides=(2, 2), activation='relu'),\n",
        "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "            tf.keras.layers.Flatten(),\n",
        "            tf.keras.layers.Dense(512, activation='relu'),\n",
        "            tf.keras.layers.Dense(self.n_actions)\n",
        "        ])\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(lr), loss='mean_squared_error')\n",
        "        return model\n",
        "\n",
        "    def sync_target(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.n_actions)\n",
        "        q_values = self.model.predict(np.expand_dims(state, axis=0))\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "        if len(self.memory) > self.memory_size:\n",
        "            self.memory.pop(0)\n",
        "\n",
        "    # def train(self):\n",
        "    #     if len(self.memory) < self.batch_size:\n",
        "    #         return\n",
        "    #     minibatch = random.sample(self.memory, self.batch_size)\n",
        "    #     states, targets = [], []\n",
        "    #     for state, action, reward, next_state, done in minibatch:\n",
        "    #         target = reward\n",
        "    #         if not done:\n",
        "    #             q_values_next_state = self.target_model.predict(np.expand_dims(next_state, axis=0))\n",
        "    #             target = reward + self.gamma * np.max(q_values_next_state[0])\n",
        "    #         print(\"State Shape:\", state.shape)\n",
        "    #         print(\"State Type:\", type(state))\n",
        "\n",
        "    #         final_target = self.model.predict(np.expand_dims(state, axis=0))\n",
        "    #         final_target[0][action] = target\n",
        "    #         states.append(state)\n",
        "    #         targets.append(final_target[0])\n",
        "    #     self.model.train_on_batch(np.array(states), np.array(targets))\n",
        "\n",
        "    #     if self.epsilon > self.epsilon_min:\n",
        "    #         self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def train(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
        "        states = np.array(states)\n",
        "        next_states = np.array(next_states)\n",
        "\n",
        "        actions = np.asarray(actions)\n",
        "\n",
        "        q_values_next_state = self.target_model.predict(next_states)\n",
        "        targets = rewards + self.gamma * np.max(q_values_next_state, axis=1) * (1 - dones)\n",
        "        targets = np.expand_dims(targets, axis=1)\n",
        "\n",
        "        # Change the type of the action variable to a numpy array\n",
        "        action = actions[:, 0]\n",
        "\n",
        "        final_target = self.model.predict(np.expand_dims(states, axis=0))\n",
        "        final_target[0][action] = targets[0]\n",
        "        self.model.train_on_batch(states, targets)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def sync_target(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # env = AtariPreprocessing(gym.make(\"BreakoutNoFrameskip-v4\"))\n",
        "    env = gym.make('Breakout-v4')\n",
        "\n",
        "    dqn = DQN(state_shape=(210, 160, 3), n_actions=env.action_space.n)\n",
        "\n",
        "    EPISODES = 500\n",
        "    SYNC_TARGET_STEPS = 1000\n",
        "    steps = 0\n",
        "\n",
        "    for e in range(EPISODES):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = dqn.act(state)\n",
        "            # output = env.step(action)\n",
        "            # print(output)\n",
        "\n",
        "            next_state, reward, done, _, info = env.step(action)\n",
        "\n",
        "            dqn.remember(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            print(\"State shape:\",np.shape(state))\n",
        "            print(type(state))\n",
        "            if isinstance(state, list):\n",
        "                for s in state:\n",
        "                    print(np.shape(s))\n",
        "                # for s in state:\n",
        "                #     prediction = self.model.predict(np.expand_dims(s, axis=0))\n",
        "\n",
        "\n",
        "            dqn.train()\n",
        "            steps += 1\n",
        "            if steps % SYNC_TARGET_STEPS == 0:\n",
        "                dqn.sync_target()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "Xt7ZJm-MN8uD",
        "outputId": "62d7290f-dc38-409f-b31c-521cf1ec00ca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.10/dist-packages (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.0.8)\n",
            "Requirement already satisfied: ale-py~=0.8.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.8.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.0->gym[atari]) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.0->gym[atari]) (4.7.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-1b72021b7c34>\u001b[0m in \u001b[0;36m<cell line: 108>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;31m# env = AtariPreprocessing(gym.make(\"BreakoutNoFrameskip-v4\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Breakout-v4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mdqn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m210\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m160\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;31m# Assume it's a string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0menv_creator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"render_mode\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \"\"\"\n\u001b[1;32m     60\u001b[0m     \u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym.envs.atari'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install keras\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "import gym\n",
        "\n",
        "# Constants\n",
        "GAMMA = 0.99\n",
        "LR_ACTOR = 0.0003\n",
        "LR_CRITIC = 0.001\n",
        "CLIPPING_LOSS_RATIO = 0.2\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "class PPO:\n",
        "    def __init__(self, state_dim, n_actions):\n",
        "        self.state_dim = state_dim\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "        self.actor = self.create_actor()\n",
        "        self.critic = self.create_critic()\n",
        "        self.old_actor = self.create_actor()\n",
        "        self.old_actor.set_weights(self.actor.get_weights())\n",
        "\n",
        "    def create_actor(self):\n",
        "        states = Input(shape=self.state_dim)\n",
        "        x = Dense(64, activation='relu')(states)\n",
        "        x = Dense(64, activation='relu')(x)\n",
        "        probs = Dense(self.n_actions, activation='softmax')(x)\n",
        "        model = keras.models.Model(inputs=states, outputs=probs)\n",
        "        return model\n",
        "\n",
        "    def create_critic(self):\n",
        "        states = Input(shape=self.state_dim)\n",
        "        x = Dense(64, activation='relu')(states)\n",
        "        x = Dense(64, activation='relu')(x)\n",
        "        values = Dense(1)(x)\n",
        "        model = keras.models.Model(inputs=states, outputs=values)\n",
        "        return model\n",
        "\n",
        "    # def get_action(self, state):\n",
        "    #     probabilities = self.actor.predict(np.expand_dims(state, axis=0))\n",
        "    #     action = np.random.choice(self.n_actions, p=probabilities[0])\n",
        "    #     return action\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = np.array(state).flatten()  # Flatten the state\n",
        "        probabilities = self.actor.predict(np.expand_dims(state, axis=0))\n",
        "        action = np.random.choice(self.n_actions, p=probabilities[0])\n",
        "        return action\n",
        "\n",
        "\n",
        "    def train(self, states, actions, rewards, next_states, dones):\n",
        "        actor_optimizer = keras.optimizers.Adam(learning_rate=LR_ACTOR)\n",
        "        critic_optimizer = keras.optimizers.Adam(learning_rate=LR_CRITIC)\n",
        "\n",
        "        old_probs = self.old_actor.predict_on_batch(states)\n",
        "        values = self.critic.predict_on_batch(states)\n",
        "        next_values = self.critic.predict_on_batch(next_states)\n",
        "        targets = rewards + GAMMA * next_values * (1-dones)\n",
        "        advantages = targets - values\n",
        "\n",
        "        for _ in range(EPOCHS):\n",
        "            for idx in range(0, len(states), BATCH_SIZE):\n",
        "                batch_states = states[idx:idx+BATCH_SIZE]\n",
        "                batch_actions = actions[idx:idx+BATCH_SIZE]\n",
        "                batch_advantages = advantages[idx:idx+BATCH_SIZE]\n",
        "                batch_old_probs = old_probs[idx:idx+BATCH_SIZE]\n",
        "\n",
        "                with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
        "                    probs = self.actor(batch_states)\n",
        "                    batch_action_probs = probs[np.arange(len(batch_states)), batch_actions]\n",
        "                    batch_old_action_probs = batch_old_probs[np.arange(len(batch_states)), batch_actions]\n",
        "                    ratios = batch_action_probs / (batch_old_action_probs + 1e-10)\n",
        "\n",
        "                    # Clipping Loss\n",
        "                    surrogate1 = ratios * batch_advantages\n",
        "                    surrogate2 = tf.clip_by_value(ratios, 1 - CLIPPING_LOSS_RATIO, 1 + CLIPPING_LOSS_RATIO) * batch_advantages\n",
        "                    actor_loss = -tf.reduce_mean(tf.minimum(surrogate1, surrogate2))\n",
        "\n",
        "                    current_values = self.critic(batch_states)\n",
        "                    critic_loss = tf.reduce_mean(tf.square(current_values - targets))\n",
        "\n",
        "                actor_grads = tape1.gradient(actor_loss, self.actor.trainable_variables)\n",
        "                critic_grads = tape2.gradient(critic_loss, self.critic.trainable_variables)\n",
        "\n",
        "                actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
        "                critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n",
        "\n",
        "        self.old_actor.set_weights(self.actor.get_weights())\n",
        "\n",
        "env = gym.make(\"Pong-v0\")\n",
        "state_dim = env.observation_space.shape\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "ppo = PPO(state_dim, n_actions)\n",
        "\n",
        "for _ in range(1000): # Number of episodes\n",
        "    state = env.reset()\n",
        "    episode_states, episode_actions, episode_rewards, episode_next_states, episode_dones = [], [], [], [], []\n",
        "\n",
        "    while True:\n",
        "        action = ppo.get_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        episode_states.append(state)\n",
        "        episode_actions.append(action)\n",
        "        episode_rewards.append(reward)\n",
        "        episode_next_states.append(next_state)\n",
        "        episode_dones.append(done)\n",
        "\n",
        "        if done:\n",
        "            ppo.train(np.array(episode_states), np.array(episode_actions), np.array(episode_rewards), np.array(episode_next_states), np.array(episode_dones))\n",
        "            break\n",
        "        state = next_state\n"
      ],
      "metadata": {
        "id": "I1OCDhOROEGC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}